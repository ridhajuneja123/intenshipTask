{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import nltk\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.pipeline import make_pipeline, Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import make_scorer, accuracy_score, f1_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, roc_auc_score, recall_score, precision_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support as score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "porterStemmer = PorterStemmer()\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/ridha/Downloads/Subtask-A-master/V1.4_Training.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.read_csv('/home/ridha/Downloads/Subtask-A-master/SubtaskA_Trial_Test_Labeled.csv',encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>663_3</td>\n",
       "      <td>\"Please enable removing language code from the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>663_4</td>\n",
       "      <td>\"Note: in your .csproj file, there is a Suppor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>664_1</td>\n",
       "      <td>\"Wich means the new version not fully replaced...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>664_2</td>\n",
       "      <td>\"Some of my users will still receive the old x...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>664_3</td>\n",
       "      <td>\"The store randomly gives the old xap or the n...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>664_4</td>\n",
       "      <td>\"My app has a WP7 version and a WP8 version XA...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>664_5</td>\n",
       "      <td>\"The wp7 xap works only on WP7 and the wp8 xap...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>665_1</td>\n",
       "      <td>\"Sometimes the Store gives the wrong wp7 xap v...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>665_2</td>\n",
       "      <td>\"It should be an option to remove the \"ru\" lan...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>665_3</td>\n",
       "      <td>\"Currently if you ever mistakenly selected a \"...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       0                                                  1  2\n",
       "0  663_3  \"Please enable removing language code from the...  1\n",
       "1  663_4  \"Note: in your .csproj file, there is a Suppor...  0\n",
       "2  664_1  \"Wich means the new version not fully replaced...  0\n",
       "3  664_2  \"Some of my users will still receive the old x...  0\n",
       "4  664_3  \"The store randomly gives the old xap or the n...  0\n",
       "5  664_4  \"My app has a WP7 version and a WP8 version XA...  0\n",
       "6  664_5  \"The wp7 xap works only on WP7 and the wp8 xap...  0\n",
       "7  665_1  \"Sometimes the Store gives the wrong wp7 xap v...  0\n",
       "8  665_2  \"It should be an option to remove the \"ru\" lan...  1\n",
       "9  665_3  \"Currently if you ever mistakenly selected a \"...  0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tried to work with train set divided into train and val set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "train, test = train_test_split(data, test_size=0.2, random_state=1)\n",
    "X_train = train[1].values\n",
    "X_test = test[1].values\n",
    "y_train = train[2]\n",
    "y_test = test[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Pipeline for svm along with grid search cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text): \n",
    "    return nltk.word_tokenize(text)\n",
    "\n",
    "def stem(doc):\n",
    "    return (stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "en_stopwords = set(stopwords.words(\"english\")) \n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    analyzer = 'word',\n",
    "    tokenizer = tokenize,\n",
    "    lowercase = True,\n",
    "    ngram_range=(1, 1),\n",
    "    stop_words = en_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = StratifiedKFold(n_splits=5, shuffle=True, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(1)\n",
    "\n",
    "pipeline_svm = make_pipeline(vectorizer, \n",
    "                            SVC(probability=True, kernel=\"rbf\", class_weight=\"balanced\"))\n",
    "\n",
    "grid_svm = GridSearchCV(pipeline_svm,\n",
    "                    param_grid = {'svc__C': [0.01, 0.1, 1]}, \n",
    "                    cv = kfolds,\n",
    "                    scoring=\"roc_auc\",\n",
    "                    verbose=1,   \n",
    "                    n_jobs=-1) \n",
    "\n",
    "grid_svm.fit(X_train, y_train)\n",
    "grid_svm.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the best params and best estimator out of grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    " grid_svm.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8084708142346081"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid_svm.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=grid_svm.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "296"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_data[test_data['label']==1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "result=a.predict(test_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classwise(y_true, y_pred):\n",
    "    return classification_report(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting the classwise accuracy results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ridha/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'              precision    recall  f1-score   support\\n\\n           0       0.00      0.00      0.00       296\\n           1       0.50      1.00      0.67       296\\n\\n   micro avg       0.50      0.50      0.50       592\\n   macro avg       0.25      0.50      0.33       592\\nweighted avg       0.25      0.50      0.33       592\\n'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classwise(test_data['label'], result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    " accuracy = accuracy_score(result, test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2085\n",
      "8500\n"
     ]
    }
   ],
   "source": [
    "print(len(data[data[2]==1]))\n",
    "print(len(data[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running svm pipeline for entire data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_svm = make_pipeline(vectorizer, \n",
    "                            SVC(probability=True, kernel=\"rbf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ridha/anaconda3/lib/python3.6/site-packages/sklearn/feature_extraction/text.py:300: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens [\"'d\", \"'ll\", \"'re\", \"'s\", \"'ve\", 'could', 'might', 'must', \"n't\", 'need', 'sha', 'wo', 'would'] not in stop_words.\n",
      "  'stop_words.' % sorted(inconsistent))\n",
      "/home/ridha/anaconda3/lib/python3.6/site-packages/sklearn/svm/base.py:196: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('countvectorizer', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None,\n",
       "        stop_words=...bf', max_iter=-1, probability=True, random_state=None,\n",
       "  shrinking=True, tol=0.001, verbose=False))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_svm.fit(data[1], data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1=pipeline_svm.predict(test_data['sentence'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(result1, test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with glove dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting glove_python\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3e/79/7e7e548dd9dcb741935d031117f4bed133276c2a047aadad42f1552d1771/glove_python-0.1.0.tar.gz (263kB)\n",
      "\u001b[K    100% |████████████████████████████████| 266kB 739kB/s ta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy in ./anaconda3/lib/python3.6/site-packages (from glove_python) (1.15.4)\n",
      "Requirement already satisfied: scipy in ./anaconda3/lib/python3.6/site-packages (from glove_python) (1.1.0)\n",
      "Building wheels for collected packages: glove-python\n",
      "  Running setup.py bdist_wheel for glove-python ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/ridha/.cache/pip/wheels/88/4b/6d/10c0d2ad32c9d9d68beec9694a6f0b6e83ab1662a90a089a4b\n",
      "Successfully built glove-python\n",
      "Installing collected packages: glove-python\n",
      "Successfully installed glove-python-0.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install glove_python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## defining my own preprocessing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Preprocessing(data):\n",
    "    sent=nltk.sent_tokenize(data)\n",
    "    token=[]\n",
    "    for sentence in sent:\n",
    "        sent1=sentence.casefold()\n",
    "        words=sent1.split()\n",
    "        for word in words: \n",
    "            word1=word.strip('<>()\\.\\,\\':\\\"\\?\\![\\]{}@\\^#*;_\\-\\+=$')\n",
    "            word2=porterStemmer.stem(word1)\n",
    "            token.append(word2)\n",
    "    return token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = []\n",
    "# idx = 0\n",
    "# word2idx = {}\n",
    "# vectors = []\n",
    "# with open('/home/ridha/Desktop/glove.6B.50d.txt', 'rb') as f:\n",
    "#     for l in f:\n",
    "#         line = l.decode().split()\n",
    "#         word = line[0]\n",
    "#         words.append(word)\n",
    "#         word2idx[word] = idx\n",
    "#         idx += 1\n",
    "#         vect = np.array(line[1:]).astype(np.float)\n",
    "#         vectors.append(vect)\n",
    "    \n",
    "# vectors = bcolz.carray(vectors[1:].reshape((400000, 50)), rootdir=f'{glove_path}/6B.50.dat', mode='w')\n",
    "# vectors.flush()\n",
    "# pickle.dump(words, open(f'{glove_path}/6B.50_words.pkl', 'wb'))\n",
    "# pickle.dump(word2idx, open(f'{glove_path}/6B.50_idx.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors = np.array(vectors)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectors=vectors.reshape(400000, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(vectors.shape)\n",
    "# glove = {w: vectors[word2idx[w]] for w in words}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f= open('glovedict','wb')\n",
    "# pickle.dump(glove,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.close()\n",
    "# glove={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "f= open('glovedict','rb')\n",
    "glove=pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.20346 , -0.36144 ,  1.065   ,  0.77532 ,  0.13116 ,  0.084812,\n",
       "       -0.09789 ,  0.058289,  0.068911, -0.29628 , -0.47937 ,  0.24026 ,\n",
       "        0.19721 ,  0.41073 ,  0.80101 , -0.21375 , -1.3117  , -0.88078 ,\n",
       "        1.3917  , -0.86331 ,  0.13966 , -0.68027 ,  0.058374,  0.8234  ,\n",
       "       -0.72267 , -1.1106  , -0.10921 , -0.27527 ,  0.035973, -0.70028 ,\n",
       "        2.5397  , -1.268   , -0.95494 , -0.66966 , -0.43851 ,  0.27497 ,\n",
       "        0.602   , -1.2052  ,  0.33294 , -0.96118 ,  1.2398  , -0.17181 ,\n",
       "        0.043022,  1.1535  , -0.64861 ,  0.10567 ,  0.38906 ,  0.47547 ,\n",
       "        0.015746,  0.017795])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove['file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Using keywords from baseline implementation adding extra importance for these words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = [\"suggest\",\"recommend\",\"hopefully\",\"go for\",\"request\",\"it would be nice\",\"adding\",\"should come with\",\"should be able\",\"could come with\", \"i need\" , \"we need\",\"needs\", \"would like to\",\"would love to\",\"allow\",\"add\"]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sentenceVector for training data getting the vector representation of word from glove and simply adding for all words in sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=list(data[1])\n",
    "corpa=[]\n",
    "for sent in a:\n",
    "    corpa.append([word for word in Preprocessing(sent)])\n",
    "corpa[1] \n",
    "sentenceVect=[]\n",
    "for sent_token in corpa:\n",
    "    vect=np.zeros(50)\n",
    "    for token in sent_token:\n",
    "        if glove.get(token) is not None :\n",
    "            vect+= glove[token]\n",
    "            if token in keywords:\n",
    "                vect+= 0.3*glove[token]\n",
    "    sentenceVect.append(vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SVC(gamma='auto',kernel=\"rbf\",C=1.0,class_weight={1: 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight={1: 3}, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#sentenceVect[1]\n",
    "clf.fit(sentenceVect,data[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910588235294118\n"
     ]
    }
   ],
   "source": [
    "result=clf.predict(sentenceVect)\n",
    "accuracy = accuracy_score(result, data[2])\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating test sentence vector  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=list(test_data['sentence'])\n",
    "corpa=[]\n",
    "for sent in a:\n",
    "    corpa.append([word for word in Preprocessing(sent)])\n",
    "corpa[1] \n",
    "sentenceVect=[]\n",
    "for sent_token in corpa:\n",
    "    vect=np.zeros(50)\n",
    "    for token in sent_token:\n",
    "        if glove.get(token) is not None:\n",
    "            vect+= glove[token]\n",
    "            if token in keywords:\n",
    "                vect+= 0.3*glove[token]\n",
    "    sentenceVect.append(vect)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## getting results for the evaluation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "result1=clf.predict(sentenceVect)\n",
    "accuracy = accuracy_score(result1, test_data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5287162162162162\n"
     ]
    }
   ],
   "source": [
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#data[1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from glove import Corpus, Glove\n",
    "\n",
    "# # creating a corpus object\n",
    "# corpus = Corpus() \n",
    "\n",
    "# #training the corpus to generate the co occurence matrix which is used in GloVe\n",
    "# corpus.fit(corpa, window=10)\n",
    "\n",
    "# #creating a Glove object which will use the matrix created in the above lines to create embeddings\n",
    "# #We can set the learning rate as it uses Gradient Descent and number of components\n",
    "\n",
    "# glove = Glove(no_components=5, learning_rate=0.05)\n",
    " \n",
    "# glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "# glove.add_dictionary(corpus.dictionary)\n",
    "# glove.save('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove.add_dictionary(corpus.dictionary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove.word_vectors[glove.dictionary['file']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating sentence vectors\n",
    "# len(glove.dictionary.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
